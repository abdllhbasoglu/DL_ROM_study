{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a7eeb5-ced4-46ce-981b-276d5fe444ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model_12 import Autoencoder\n",
    "from utils_V2 import Reconstructed_writer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from torch.utils.data import random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# a seed value is assigned to control randomization\n",
    "my_seed = 24\n",
    "def set_seed (my_seed = 24):\n",
    "  np.random.seed(my_seed)\n",
    "  torch.manual_seed(my_seed)\n",
    "  torch.cuda.manual_seed(my_seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(my_seed=my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943910e-9420-403b-8639-1a454694967d",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60253635-43f2-4c77-ad3e-765312b8815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "(50, 3, 150, 498)\n",
      "(50, 3, 150, 498)\n",
      "(50, 150, 498)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "data = np.load(\"flowfield.npy\")\n",
    "minibatch = 5 # # how many samples per batch to load\n",
    "print(data.shape)\n",
    "if minibatch == None:\n",
    "  minibatch = data.shape[0]\n",
    "\n",
    "# uploading the data from numpy file\n",
    "\n",
    "flow_cond = np.load(\"flowcon.npy\")\n",
    "x_coordinates = np.load(\"x_coordinate.npy\")\n",
    "y_coordinates = np.load(\"y_coordinate.npy\")\n",
    "\n",
    "print(data.shape)\n",
    "#data = data.to(device)\n",
    "print(x_coordinates.shape)\n",
    "#to split randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce2caab-2db3-4798-a8ff-fb08e69163d4",
   "metadata": {},
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c43fb8-b250-4714-9aed-2d184ca8240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 150, 498)\n"
     ]
    }
   ],
   "source": [
    "# normalizing the flow field\n",
    "flowfield_mean = np.mean(data, axis=0) # cell-based mean values\n",
    "flowfield_std = np.std(data, axis=0) # cell-based std values\n",
    "print(flowfield_mean.shape)\n",
    "\n",
    "normalized_data = (data - flowfield_mean) / flowfield_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57579381-1c81-4396-9580-12479e1aa7ab",
   "metadata": {},
   "source": [
    "# Split and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b658c738-66d9-4938-906b-7e77799d3ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 150, 498)\n"
     ]
    }
   ],
   "source": [
    "# splitting train and validation data (coordinates, AoA whole together shuffled)\n",
    "train_data, val_data, train_xcoord, test_xcoord, train_ycoord, test_ycoord, train_flowcon, val_flowcon = train_test_split(normalized_data, x_coordinates, y_coordinates, flow_cond, test_size=0.2)\n",
    "print(train_data[1].shape)\n",
    "\n",
    "# make numpy to tensor\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "train_loader = DataLoader(torch.tensor(train_data), batch_size=minibatch, shuffle=False)\n",
    "val_loader = DataLoader(torch.tensor(val_data), batch_size=val_data.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700cf55-f39a-419e-967e-b62d70edf407",
   "metadata": {},
   "source": [
    "# Training preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22206d5d-fa08-4b5b-9884-76e85305d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1050 Ti\n",
      "Toplam parametre sayısı: 1444003\n"
     ]
    }
   ],
   "source": [
    "# Root mean square error\n",
    "# Root mean square error\n",
    "criterion = nn.MSELoss()\n",
    "def RMSELoss(recon_x,x):\n",
    "    return torch.sqrt(criterion(recon_x,x))\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 300\n",
    "\n",
    "lr_step_size = 150\n",
    "lr_gamma = 0.2\n",
    "set_seed (my_seed)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "model = Autoencoder().to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Toplam parametre sayısı: {total_params}') # to see total parameters\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, lr_step_size, gamma=lr_gamma)\n",
    "start = time.time()\n",
    "min_val_loss = np.Inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46497b0-71eb-4f86-9dd5-6be1d84d08b7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a620ca4-84cf-4be3-ad42-4adbce594406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:26<00:26, 26.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/2] Time: 61 Loss: 1.040751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:40<00:40, 40.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# train the model #\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (images) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m---> 14\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     recon_images \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m RMSELoss(recon_images, images)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#val_losses = []\n",
    "loss_his = []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    #monitor training loss and validation loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Empty the outputs list at the beginning of each epoch\n",
    "    outputs = []\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for idx, (images) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        recon_images = model(images)\n",
    "        loss = RMSELoss(recon_images, images)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs.append(recon_images)\n",
    "    loss_his.append(loss.item())\n",
    "    scheduler.step()\n",
    "          \n",
    "    \n",
    "\n",
    "    if epoch % 10 == 0:# and epoch != 0:\n",
    "      \n",
    "      to_print = \"Epoch[{}/{}] Time: {:.0f} Loss: {:.6f}\".format(epoch+1, \n",
    "                              n_epochs, time.time()-start, loss.item())\n",
    "      print(to_print)    \n",
    "\n",
    "# concatenate outputs list to get the final output tensor\n",
    "final_output = torch.cat(outputs, dim=0)\n",
    "print(final_output[1,0,:,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0ce90-1e52-423c-a951-761b37c5732a",
   "metadata": {},
   "source": [
    "# Inference Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b227bcd-1eed-4a21-807d-c184e9a9a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images in val_loader:\n",
    "      images = images.to(device)\n",
    "      recon_images_val = model(images)\n",
    "      loss_val_p = RMSELoss(images[:,0], recon_images_val[:,0])\n",
    "      loss_val_u = RMSELoss(images[:,1], recon_images_val[:,1])\n",
    "      loss_val_v = RMSELoss(images[:,2], recon_images_val[:,2])\n",
    "      print(loss_val_p)\n",
    "      print(loss_val_u)\n",
    "      print(loss_val_v)\n",
    "\n",
    "# concatenate outputs list to get the final output tensor\n",
    "final_output_val = torch.cat((recon_images_val,), dim=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f3499-62f9-4a91-900f-d522acf45bf7",
   "metadata": {},
   "source": [
    "# Reconstruction the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46ab19-8430-4e6a-abee-df19a70ae0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing normalized data back to orginal distribution for training dataset\n",
    "reconstructed_data = torch.zeros(len(train_data),3,150,498)\n",
    "for i in range(3):\n",
    "    for k in range(len(train_data)):\n",
    "        reconstructed_data[k,i,:,:] = final_output[k,i,:,:]* torch.tensor(flowfield_std).to(device)[i] + torch.tensor(flowfield_mean).to(device)[i]\n",
    "        \n",
    "# Origininal data\n",
    "original_data = torch.zeros(len(train_data),3,150,498)\n",
    "for i in range(3):\n",
    "    for k in range(len(train_data)):\n",
    "        original_data[k,i,:,:] = torch.tensor(train_data[k,i,:,:]).to(device)* torch.tensor(flowfield_std).to(device)[i] + torch.tensor(flowfield_mean).to(device)[i]\n",
    "    \n",
    "# error data\n",
    "error_data = original_data - reconstructed_data\n",
    "\n",
    "# changing normalized data back to orginal distribution for validation dataset\n",
    "reconstructed_val = torch.zeros(len(val_data),3,150,498)\n",
    "for i in range(3):\n",
    "    for k in range(len(val_data)):\n",
    "        reconstructed_val[k,i,:,:] = final_output_val[k,i,:,:]* torch.tensor(flowfield_std).to(device)[i] + torch.tensor(flowfield_mean).to(device)[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14364d5-e8e2-4cab-82bb-82ddd6c0a31e",
   "metadata": {},
   "source": [
    "# Exporting the Reconstructed Flow Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b570334-0724-41f1-8e1f-33262837c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of write files\n",
    "phat_file = r\"C:\\Users\\Abdullah\\Desktop\\ASDL\\Optimization_Team\\Toy_Problem\\Model\\output\\v4\\P\"\n",
    "uhat_file = r\"C:\\Users\\Abdullah\\Desktop\\ASDL\\Optimization_Team\\Toy_Problem\\Model\\output\\v4\\u\"\n",
    "vhat_file = r\"C:\\Users\\Abdullah\\Desktop\\ASDL\\Optimization_Team\\Toy_Problem\\Model\\output\\v4\\v\"\n",
    "\n",
    "#error file locations\n",
    "pe_file = r\"C:\\Users\\Abdullah\\Desktop\\ASDL\\Optimization_Team\\Toy_Problem\\Model\\output\\v4\\error\\P_error\"\n",
    "ue_file = r\"C:\\Users\\Abdullah\\Desktop\\ASDL\\Optimization_Team\\Toy_Problem\\Model\\output\\v4\\error\\u_error\"\n",
    "ve_file = r\"C:\\Users\\Abdullah\\Desktop\\ASDL\\Optimization_Team\\Toy_Problem\\Model\\output\\v4\\error\\v_error\"\n",
    "\n",
    "# writing constructed data to .dat files / train data\n",
    "train_write = Reconstructed_writer(dataset=reconstructed_data, flow_cond = flow_cond, flowcon = train_flowcon, phat_file=phat_file, uhat_file=uhat_file, vhat_file=vhat_file, num_cases=len(reconstructed_data), num_rows=150, num_cols=498, num_channels=3) # num_channels-1 due to AoA as channel\n",
    "export_train = train_write.write_dataset()\n",
    "\n",
    "# writing error to .dat files / error data\n",
    "error_write = Reconstructed_writer(dataset=error_data, flow_cond = flow_cond, flowcon = train_flowcon, phat_file=pe_file, uhat_file=ue_file, vhat_file=ve_file, num_cases=len(reconstructed_data), num_rows=150, num_cols=498, num_channels=3) # num_channels-1 due to AoA as channel\n",
    "export_error = error_write.write_dataset()\n",
    "\n",
    "# writing constructed data to .dat files / validation data\n",
    "val_write = Reconstructed_writer(dataset=reconstructed_val, flow_cond = flow_cond, flowcon = val_flowcon, phat_file=phat_file, uhat_file=uhat_file, vhat_file=vhat_file, num_cases=len(reconstructed_val), num_rows=150, num_cols=498, num_channels=3) # num_channels-1 due to AoA as channel\n",
    "export_val = val_write.write_dataset()\n",
    "\n",
    "# Save the model\n",
    "#torch.save(model.state_dict(), r'C:\\Users\\Abdullah\\Desktop\\ASDL\\Optimization_Team\\Toy Problem\\Model\\conv_autoencoder.pth')\n",
    "\n",
    "# Load the state of the model from the conv_autoencoder file\n",
    "# model.load_state_dict(torch.load(r'C:\\Users\\Abdullah\\Desktop\\ASDL\\Optimization_Team\\Toy Problem\\Model\\conv_autoencoder.pth'))\n",
    "\n",
    " # Plotting the training and validation losses\n",
    " \n",
    "#plt.plot(range(1, n_epochs+1), train_losses, label='Training Loss')\n",
    "#plt.plot(range(1, n_epochs+1), val_losses, label='Validation Loss')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Loss')\n",
    "#plt.title('Training and Validation Losses')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
